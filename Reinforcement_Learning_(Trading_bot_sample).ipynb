{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyM2CSIQV4LdEk5MZDdqMPrC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manny-uncharted/pytorch-projects-learnings/blob/main/Reinforcement_Learning_(Trading_bot_sample).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L-TIriidYJ8j"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zifiMklM--8g"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class TradingAgent:\n",
        "    def __init__(self, symbol, window_size, episode_length, capital, num_shares, max_position, data):\n",
        "        self.symbol = symbol\n",
        "        self.window_size = window_size\n",
        "        self.episode_length = episode_length\n",
        "        self.max_position = max_position\n",
        "        self.capital = capital\n",
        "        self.num_shares = num_shares\n",
        "        self.data = data  # store the stock price history in the 'data' attribute\n",
        "        self.stock_price_history = []\n",
        "        self.state = None\n",
        "    \n",
        "    def reset(self):\n",
        "        self.capital = 100  # reset the capital to its initial value\n",
        "        self.num_shares = 10  # reset the number of shares to its initial value\n",
        "        self.stock_price_history = []  # reset the stock price history\n",
        "        self.state = None  # reset the state variable\n",
        "\n",
        "    def act(self, state, epsilon):\n",
        "        \"\"\"\n",
        "        This method takes in a state and an epsilon value and returns the action\n",
        "        to take. With probability epsilon, it will return a random action.\n",
        "        Otherwise, it will return the action with the highest Q-value.\n",
        "        \"\"\"\n",
        "        if np.random.uniform(0, 1) < epsilon:\n",
        "            return np.random.randint(-self.max_position, self.max_position + 1)\n",
        "        else:\n",
        "            return np.argmax(state)\n",
        "\n",
        "    def reward(self, state, action, next_state):\n",
        "      \"\"\"\n",
        "      This method takes in a state, an action, and a next state and returns\n",
        "      the reward for taking that action.\n",
        "      \"\"\"\n",
        "      if next_state is None:\n",
        "          return 1\n",
        "      elif action > 0:\n",
        "          return (next_state[0] - state[0]) * state[1]\n",
        "      else:\n",
        "          return (state[0] - next_state[0]) * state[1]\n",
        "\n",
        "\n",
        "    def get_state(self, t):\n",
        "      \"\"\"\n",
        "      This method takes in a time t and returns the state at that time.\n",
        "      \"\"\"\n",
        "      if t < self.window_size:\n",
        "          return None\n",
        "      else:\n",
        "          window = self.data.iloc[t - self.window_size:t]\n",
        "          return (window.iloc[-1]['close'],\n",
        "                  window.mean()['close'],\n",
        "                  window.std()['close'])\n",
        "\n",
        "\n",
        "    # def train(self, Q, alpha=0.2, gamma=0.9, epsilon=0.1, episodes=50):\n",
        "    #     \"\"\"\n",
        "    #     This method trains the agent over a number of episodes.\n",
        "    #     \"\"\"\n",
        "    #     for episode in range(episodes):\n",
        "    #         state = self.get_state(0)\n",
        "    #         total_reward = 0\n",
        "    #         for t in range(1, self.episode_length):\n",
        "    #             action = self.act(state, epsilon)\n",
        "    #             next_state = self.get_state(t)\n",
        "    #             reward = self.reward(state, action, next_state)\n",
        "    #             total_reward += reward\n",
        "    #             next_action = self.act(next_state, epsilon)\n",
        "    #             Q[state][action] = Q[state][action] + alpha * (reward + gamma * Q[next_state][next_action] - Q[state][action])\n",
        "    #             state = next_state\n",
        "    #         print('Total reward for episode {}: {}'.format(episode, total_reward))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class QLearningAgent:\n",
        "    def __init__(self, alpha, gamma, epsilon, max_position):\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.max_position = max_position\n",
        "        self.Q = defaultdict(lambda: [0] * (2 * max_position + 1))\n",
        "      \n",
        "        # Initialize Q-table with zeros for all possible state-action pairs\n",
        "        for i in range(-self.max_position, self.max_position + 1):\n",
        "            for j in range(-1, 2):\n",
        "                self.Q[(i, j)] = [0, 0]\n",
        "        \n",
        "    def act(self, state, epsilon):\n",
        "        if np.random.uniform(0, 1) < epsilon:\n",
        "            return np.random.randint(-self.max_position, self.max_position + 1)\n",
        "        else:\n",
        "            return np.argmax(self.Q[state])\n",
        "\n",
        "    def get_action(self, state):\n",
        "        state_tuple = tuple(state)\n",
        "        if state_tuple not in self.Q:\n",
        "            self.Q[state_tuple] = [0, 0, 0]\n",
        "        if random.uniform(0, 1) < self.epsilon:\n",
        "            action = np.random.choice(self.actions)\n",
        "        else:\n",
        "            q_values = self.Q[state_tuple]\n",
        "            max_q_value = max(q_values)\n",
        "            count = q_values.count(max_q_value)\n",
        "            if count > 1:\n",
        "                best_actions = [i for i in range(len(self.actions)) if q_values[i] == max_q_value]\n",
        "                action_index = np.random.choice(best_actions)\n",
        "            else:\n",
        "                action_index = q_values.index(max_q_value)\n",
        "            action = self.actions[action_index]\n",
        "        return action\n",
        "\n",
        "    def reward(self, state, action, next_state):\n",
        "      if state is None or next_state is None:\n",
        "          return 0\n",
        "      elif action > 0:\n",
        "          return (next_state[0] - state[0]) * state[1]\n",
        "      else:\n",
        "          return (state[0] - next_state[0]) * state[1]\n",
        "\n",
        "\n",
        "    def learn(self, state, action, reward, next_state, done):\n",
        "        next_action = self.act(next_state, self.epsilon)\n",
        "        td_target = reward + self.gamma * self.Q[next_state][next_action]\n",
        "        td_error = td_target - self.Q[state][action]\n",
        "        self.Q[state][action] += self.alpha * td_error\n",
        "\n",
        "    def update_q_table(self, state, action, reward, next_state):\n",
        "        state_tuple = tuple(state.tolist())\n",
        "        next_state_tuple = tuple(next_state.tolist())\n",
        "        if next_state_tuple not in self.q_table:\n",
        "            self.q_table[next_state_tuple] = [0, 0, 0]\n",
        "        current_q = self.q_table[state_tuple][action]\n",
        "        max_q = max(self.q_table[next_state_tuple])\n",
        "        new_q = current_q + self.alpha * (reward + self.gamma * max_q - current_q)\n",
        "        self.q_table[state_tuple][action] = new_q\n",
        "\n",
        "    def decay_epsilon(self, factor):\n",
        "        self.epsilon *= factor\n",
        "\n",
        "\n",
        "def train(trading_agent, q_agent, num_episodes):\n",
        "    Q = q_agent.Q\n",
        "    for episode in range(num_episodes):\n",
        "        trading_agent.reset()\n",
        "        state = trading_agent.get_state(0)\n",
        "        total_reward = 0\n",
        "        for t in range(1, trading_agent.episode_length):\n",
        "            action = trading_agent.act(state, q_agent.epsilon)\n",
        "            next_state = trading_agent.get_state(t)\n",
        "            reward = trading_agent.reward(state, action, next_state)\n",
        "            total_reward += reward\n",
        "            next_action = trading_agent.act(next_state, q_agent.epsilon)\n",
        "            Q[state][action] = Q[state][action] + q_agent.alpha * (reward + q_agent.gamma * Q[next_state][next_action] - Q[state][action])\n",
        "            state = next_state\n",
        "        print('Total reward for episode {}: {}'.format(episode, total_reward))\n",
        "    return Q\n",
        "\n",
        "\n",
        "def test(trading_agent, q_agent):\n",
        "    trading_agent.reset()\n",
        "    state = trading_agent.get_state()\n",
        "    done = False\n",
        "    while not done:\n",
        "        action = q_agent.get_action(state)\n",
        "        next_state, reward, done = trading_agent.step(action)\n",
        "        state = next_state\n",
        "    return trading_agent.balance\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "\n",
        "def download_data(symbol, start_date, end_date):\n",
        "    \"\"\"\n",
        "    This function downloads stock price data from Yahoo Finance for the given\n",
        "    symbol and date range.\n",
        "    \"\"\"\n",
        "    # Download the data from Yahoo Finance\n",
        "    data = yf.download(symbol, start=start_date, end=end_date)\n",
        "    \n",
        "    # Keep only the 'Close' column\n",
        "    data = pd.DataFrame(data['Close'])\n",
        "    \n",
        "    # Rename the column to 'close'\n",
        "    data.columns = ['close']\n",
        "    \n",
        "    # Add a new column for the date\n",
        "    data['date'] = data.index\n",
        "    \n",
        "    # Reset the index\n",
        "    data = data.reset_index(drop=True)\n",
        "    \n",
        "    return data"
      ],
      "metadata": {
        "id": "dVNAzAtEI-5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = download_data('AAPL', '2020-01-01', '2022-04-06')\n"
      ],
      "metadata": {
        "id": "i9U3P5_nJEr3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trading_agent = TradingAgent('AAPL', 10, 100, 1000, 10, 5, data)\n",
        "q_agent = QLearningAgent(0.2, 0.9, 0.1, 100)\n",
        "trades = train(trading_agent, q_agent, num_episodes=50)\n"
      ],
      "metadata": {
        "id": "TYR9oVZ3F_Za"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}